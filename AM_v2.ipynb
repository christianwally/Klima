{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pickle \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#from eofs.xarray import Eof\n",
    "from eofs.multivariate.iris import MultivariateEof\n",
    "import iris\n",
    "#import iris.plot as iplt\n",
    "#import iris.quickplot as qplt\n",
    "import datetime\n",
    "\n",
    "anomalies_norm = xr.open_dataset(\"/home/srvx11/lehre/users/a1204101/AnalogMethod/data/ERA5/anom_all_1979-2018.nc\",chunks={'time': 5000, 'lat':141, 'lon': 141})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame(columns=['search_day', 'found_day'])\n",
    "N_list=np.array([])\n",
    "var_f_list=np.array([])\n",
    "for doy in range(1,367):   # Loop über alle 366 dayofyears wenn: range(1,367):\n",
    "    print('berechne doy: ', doy) # = Target Day\n",
    "    \n",
    "    # Bestimmen der benötigten doy im Zeitfenster\n",
    "    days = np.arange(doy-10,doy+11)\n",
    "    days[days<=0] = days[days<=0]+366\n",
    "    days[days>366] = days[days>366]-366\n",
    "    \n",
    "    # Ausschneiden der 21 Tage (in allen Jahren), Größe: 21 Tage x 40 Jahre x 141x141\n",
    "    anomalies_i_q = anomalies_norm.q.where(anomalies_norm.q.dayofyear.isin(days),drop=True).to_iris()\n",
    "    anomalies_i_r = anomalies_norm.r.where(anomalies_norm.r.dayofyear.isin(days),drop=True).to_iris()\n",
    "    anomalies_i_msl = anomalies_norm.msl.where(anomalies_norm.msl.dayofyear.isin(days),drop=True).to_iris()\n",
    "    \n",
    "    # solver-Objekt erzeugen\n",
    "    solver = MultivariateEof([anomalies_i_q,anomalies_i_r,anomalies_i_msl])\n",
    "    \n",
    "    # immer mehr PCs bis 90% erklärte Varianz:\n",
    "    N = 1\n",
    "    var_f = 0\n",
    "    while var_f < .9:\n",
    "        var_f = np.sum(solver.varianceFraction(neigs=N).data)\n",
    "        N = N+1\n",
    "    N_list = np.append(N_list,N)\n",
    "    var_f_list = np.append(var_f_list,var_f)\n",
    "    pcs = solver.pcs(npcs=N)\n",
    "    pcs_x = xr.DataArray.from_iris(pcs) # Umwandeln der PCs in xarray\n",
    "    del pcs    \n",
    "        \n",
    "    for year in np.arange(1979,2019): # Loop über die 40 Jahre\n",
    "        \n",
    "        # Ausschneiden der einen benötigten Analyse (= ein Tag), Größe: 141x141\n",
    "        single_q = anomalies_norm.q.sel(time=((anomalies_norm.time.dt.year == year)&(anomalies_norm.dayofyear == doy))).to_iris()\n",
    "        single_r = anomalies_norm.r.sel(time=((anomalies_norm.time.dt.year == year)&(anomalies_norm.dayofyear == doy))).to_iris()\n",
    "        single_msl = anomalies_norm.msl.sel(time=((anomalies_norm.time.dt.year == year)&(anomalies_norm.dayofyear == doy))).to_iris()\n",
    "        \n",
    "        if single_q.coord('time').points.size > 0:  # nur wenn der doy in dem Jahr existiert (doy 366 nur in Schaltjahren)\n",
    "            \n",
    "            # Berechnen der Pseudo-PCs für den einen Tag\n",
    "            pseudo_pcs = solver.projectField([single_q, single_r, single_msl], neofs=N)\n",
    "        \n",
    "            # Minimieren der Norm (=Suchen des analogen Tags = 'found_day')\n",
    "            found_idx=np.argmin(np.sum(np.sqrt((pcs_x.sel(time=~(pcs_x.time.dt.year == year)).data - pseudo_pcs.data)**2),axis=1)) # Index im pcs\n",
    "        \n",
    "            search_day = anomalies_norm.sel(time=((anomalies_norm.time.dt.year == year)&(anomalies_norm.dayofyear == doy))).time[0].data # 'aktueller Tag'\n",
    "            found_day = pcs_x.sel(time=~(pcs_x.time.dt.year == year)).isel(time=found_idx).time.data\n",
    "            \n",
    "            result = result.append(pd.DataFrame([[pd.to_datetime(search_day),pd.to_datetime(found_day)]],columns=['search_day', 'found_day']))\n",
    "\n",
    "result = result.sort_values('search_day') # richtige Reihenfolge machen\n",
    "#result.reset_index(drop=True) # nur formal\n",
    "\n",
    "pickle.dump(result, open( \"save_found_dates.p\", \"wb\" ) )\n",
    "# öffnen mit XXX = pickle.load( open( \"save_found_dates.p\", \"rb\" ) )\n",
    "pickle.dump(N_list, open( \"save_N_list.p\", \"wb\" ) )\n",
    "pickle.dump(var_f_list, open( \"save_var_f_list.p\", \"wb\" ) )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
