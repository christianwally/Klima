{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pickle \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#from eofs.xarray import Eof\n",
    "from eofs.multivariate.iris import MultivariateEof\n",
    "import iris\n",
    "#import iris.plot as iplt\n",
    "#import iris.quickplot as qplt\n",
    "import datetime\n",
    "\n",
    "anomalies_norm = xr.open_dataset(\"/home/srvx11/lehre/users/a1204101/AnalogMethod/data/ERA5/anom_all_1979-2018.nc\",chunks={'time': 5000, 'lat':141, 'lon': 141})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "berechne doy:  1\n",
      "berechne doy:  2\n",
      "   search_day  found_day  index\n",
      "0  1979-01-01 2003-01-04   8404\n",
      "0  1979-01-02 1995-01-12   5490\n",
      "0  1980-01-01 1981-01-05    369\n",
      "0  1980-01-02 1985-01-02   1827\n",
      "0  1981-01-01 2005-01-02   9133\n",
      "0  1981-01-02 1986-12-28   2553\n",
      "0  1982-01-01 2010-01-01  10958\n",
      "0  1982-01-02 2011-01-12  11334\n",
      "0  1983-01-01 2005-01-01   9132\n",
      "0  1983-01-02 2007-01-06   9867\n",
      "0  1984-01-01 1980-12-31    730\n",
      "0  1984-01-02 1988-12-23   2933\n",
      "0  1985-01-01 1980-01-02    366\n",
      "0  1985-01-02 1980-01-02    366\n",
      "0  1986-01-01 1985-12-31   2556\n",
      "0  1986-01-02 1994-01-04   5117\n",
      "0  1987-01-01 1986-12-31   2921\n",
      "0  1987-01-02 2002-12-31   8400\n",
      "0  1988-01-01 1998-01-03   6576\n",
      "0  1988-01-02 1994-12-29   5475\n",
      "0  1989-01-01 1988-12-31   3652\n",
      "0  1989-01-02 2016-12-29  13512\n",
      "0  1990-01-01 1989-12-31   4017\n",
      "0  1990-01-02 1992-12-29   4746\n",
      "0  1991-01-01 1999-01-03   6942\n",
      "0  1991-01-02 2004-01-12   8777\n",
      "0  1992-01-01 2005-01-06   9136\n",
      "0  1992-01-02 2017-12-24  13871\n",
      "0  1993-01-01 1991-12-31   4747\n",
      "0  1993-01-02 1992-12-29   5111\n",
      "..        ...        ...    ...\n",
      "0  2004-01-01 1996-01-02   6210\n",
      "0  2004-01-02 1987-01-03   2924\n",
      "0  2005-01-01 2000-01-02   7671\n",
      "0  2005-01-02 1981-01-01    731\n",
      "0  2006-01-01 1997-12-27   6935\n",
      "0  2006-01-02 1982-12-25   1454\n",
      "0  2007-01-01 1991-01-11   4393\n",
      "0  2007-01-02 2005-01-02   9498\n",
      "0  2008-01-01 2007-12-31  10591\n",
      "0  2008-01-02 2015-12-28  13144\n",
      "0  2009-01-01 2001-01-11   8046\n",
      "0  2009-01-02 2011-01-02  11324\n",
      "0  2010-01-01 1982-01-01   1096\n",
      "0  2010-01-02 1987-01-06   2927\n",
      "0  2011-01-01 2003-01-06   8771\n",
      "0  2011-01-02 2009-01-02  10959\n",
      "0  2012-01-01 2004-01-11   9141\n",
      "0  2012-01-02 1980-12-31    730\n",
      "0  2013-01-01 2018-01-01  13880\n",
      "0  2013-01-02 1989-01-07   3659\n",
      "0  2014-01-01 1993-12-29   5476\n",
      "0  2014-01-02 1993-12-30   5477\n",
      "0  2015-01-01 2011-12-26  12047\n",
      "0  2015-01-02 1983-01-02   1462\n",
      "0  2016-01-01 2008-01-03  10594\n",
      "0  2016-01-02 1986-01-09   2565\n",
      "0  2017-01-01 2014-12-24  13141\n",
      "0  2017-01-02 1988-12-25   3646\n",
      "0  2018-01-01 1998-01-05   6944\n",
      "0  2018-01-02 1991-01-05   4387\n",
      "\n",
      "[80 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "result = pd.DataFrame(columns=['search_day', 'found_day', 'index'])\n",
    "for doy in range(1,3):   # Loop über alle 366 dayofyears wenn: range(1,367):\n",
    "    print('berechne doy: ', doy) # = Target Day\n",
    "    \n",
    "    # Bestimmen der benötigten doy im Zeitfenster\n",
    "    days = np.arange(doy-10,doy+11)\n",
    "    days[days<=0] = days[days<=0]+366\n",
    "    days[days>366] = days[days>366]-366\n",
    "    \n",
    "    # Ausschneiden der 21 Tage (in allen Jahren), Größe: 21 Tage x 40 Jahre x 141x141\n",
    "    anomalies_i_q = anomalies_norm.q.where(anomalies_norm.q.dayofyear.isin(days),drop=True).to_iris()\n",
    "    anomalies_i_r = anomalies_norm.r.where(anomalies_norm.r.dayofyear.isin(days),drop=True).to_iris()\n",
    "    anomalies_i_msl = anomalies_norm.msl.where(anomalies_norm.msl.dayofyear.isin(days),drop=True).to_iris()\n",
    "    \n",
    "    # solver-Objekt erzeugen\n",
    "    solver = MultivariateEof([anomalies_i_q,anomalies_i_r,anomalies_i_msl])\n",
    "    \n",
    "    # immer mehr PCs bis 90% erklärte Varianz:\n",
    "    N = 1\n",
    "    var_f = 0\n",
    "    while var_f < .9:\n",
    "        var_f = np.sum(solver.varianceFraction(neigs=N).data)\n",
    "        N = N+1\n",
    "    pcs = solver.pcs(npcs=N)\n",
    "    pcs_x = xr.DataArray.from_iris(pcs) # Umwandeln der PCs in xarray\n",
    "    del pcs    \n",
    "        \n",
    "    for year in np.arange(1979,2019): # Loop über die 40 Jahre\n",
    "        \n",
    "        # Ausschneiden der einen benötigten Analyse (= ein Tag), Größe: 141x141\n",
    "        single_q = anomalies_norm.q.sel(time=((anomalies_norm.time.dt.year == year)&(anomalies_norm.dayofyear == doy))).to_iris()\n",
    "        single_r = anomalies_norm.r.sel(time=((anomalies_norm.time.dt.year == year)&(anomalies_norm.dayofyear == doy))).to_iris()\n",
    "        single_msl = anomalies_norm.msl.sel(time=((anomalies_norm.time.dt.year == year)&(anomalies_norm.dayofyear == doy))).to_iris()\n",
    "\n",
    "        # Berechnen der Pseudo-PCs für den einen Tag\n",
    "        pseudo_pcs = solver.projectField([single_q, single_r, single_msl], neofs=N)\n",
    "        \n",
    "        # Minimieren der Norm (=Suchen des analogen Tags = 'found_day')\n",
    "        found_idx=np.argmin(np.sum(np.sqrt((pcs_x.sel(time=~(pcs_x.time.dt.year == year)).data - pseudo_pcs.data)**2),axis=1)) # Index im pcs\n",
    "        found_total_idx=anomalies_i_q.coord('time').points[found_idx] # Index im gesamter Anomalies\n",
    "        \n",
    "        search_day = anomalies_norm.sel(time=((anomalies_norm.time.dt.year == year)&(anomalies_norm.dayofyear == doy))).time[0].data # 'aktueller Tag'\n",
    "        found_day = pcs_x.sel(time=~(pcs_x.time.dt.year == year)).isel(time=found_idx).time.data\n",
    "        \n",
    "        result = result.append(pd.DataFrame([[pd.to_datetime(search_day),pd.to_datetime(found_day),found_total_idx]],columns=['search_day', 'found_day', 'index']))\n",
    "\n",
    "print(result.sort_values('search_day'))\n",
    "pickle.dump(result.sort_values('search_day'), open( \"save_found_dates.p\", \"wb\" ) )\n",
    "# öffnen mit XXX = pickle.load( open( \"save_found_dates.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
